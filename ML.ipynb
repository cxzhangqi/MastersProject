{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1,\n",
       "              solver='lbfgs')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[0., 0.], [1., 1.]])\n",
    "y = [0, 1]\n",
    "\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 5), (5, 2), (2, 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coefs_\n",
    "[coef.shape for coef in clf.coefs_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.14962269,  0.75950271, -0.5472481 ,  6.92417703, -0.87510813]),\n",
       " array([-0.47635084, -0.76834882]),\n",
       " array([8.53354251])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.intercepts_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regressor**\n",
    "\n",
    "Class **MLPRegressor** implements a MLP that trains using backpropagation with NO activation function in the output layer. (AKA identity function as activation function).\n",
    "\n",
    "It uses the square error as the loss function, and the output is a set of continuous values.\n",
    "\n",
    "Also supports multi-output regression, in which samples can have more than one target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classifier**\n",
    "\n",
    "Class MLPClassifier implements a MLP algorithm that trains using backpropagation.\n",
    "\n",
    "Supports multi-label and multi-class (softmax) classification.\n",
    "\n",
    "Both train on\n",
    "X - (n_samples, n_features) training samples\n",
    "y - (n_samples) target values (class labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularization**\n",
    "\n",
    "Both use parameter **alpha** for regularization, avoiding overfitting by penalizing weights with large magnitudes. Can vary this with MLP:\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_alpha.html#sphx-glr-auto-examples-neural-networks-plot-mlp-alpha-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimization**\n",
    "\n",
    "Uses SGD, Adam or L-BFGS. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scaling**\n",
    "\n",
    "* Scale data as MLP sensitive to feature scaling\n",
    "i.e. standardise to have 0 mean and 1 variance, or place attribute between 0 and 1 or -1 and 1.\n",
    "\n",
    "Can use the **StandardScaler** to do this.\n",
    "\n",
    "* Learning parameter alpha\n",
    "\n",
    "Use Grid-SearchCV to find alpha usually in the range 10.0 ** -np.arrange(1,7)\n",
    "\n",
    "* L-BFGS converges quick with better solutions on small datasets. For larger use Adam. SGD with momentum or nesterov's momentum can perform better if learning rate is correctly tuned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic End-End Scikit-learn workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import dataset and save to a dataframe\n",
    "#data_df = pd.read_csv()\n",
    "\n",
    "# Group data into features and labels\n",
    "#X = data_df.drop(\"target\",axis=1)\n",
    "#y = data_df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure out which model to use\n",
    "\n",
    "https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n",
    "\n",
    "or \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an instance of the chosen model\n",
    "#clf = ...\n",
    "\n",
    "# Fit model to data\n",
    "#clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate predictions\n",
    "#clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a dataset class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# For regression:\n",
    "class Data(Dataset):\n",
    "    \n",
    "    # Initialisation\n",
    "    def __init__(self, train = True):\n",
    "            self.x = torch.arange(-3, 3, 0.1).view(-1, 1)\n",
    "            self.f = -3 * self.x + 1\n",
    "            self.y = self.f + 0.1 * torch.randn(self.x.size())\n",
    "            self.len = self.x.shape[0]\n",
    "            \n",
    "            #outliers \n",
    "            if train == True:\n",
    "                self.y[0] = 0\n",
    "                self.y[50:55] = 20\n",
    "            else:\n",
    "                pass\n",
    "      \n",
    "    # Indexer\n",
    "    def __getitem__(self, index):    \n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    # Length\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "# Create training dataset and validation dataset\n",
    "train_data = Data()\n",
    "val_data = Data(train = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import dataset into a dataframe\n",
    "#dataset = pd.read_csv()\n",
    "\n",
    "# Have a look\n",
    "#print(dataset.shape)\n",
    "#dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory analysis - PLOTS?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pre-processing\n",
    "# Two types of information, categorical (discrete)\n",
    "# or continuous\n",
    "categorical_cols = [\"columns...\"]\n",
    "numerical_cols = [\"cols\"]\n",
    "outputs = [\"cols\"]\n",
    "\n",
    "#check data types of columns\n",
    "#dataset.dtypes\n",
    "\n",
    "# Recast as category type (first step in numerical conversion)\n",
    "for category in categorical_cols:\n",
    "    dataset[category] = dataset[category].astype('category')\n",
    "\n",
    "# Convert our columns to numpy arrays\n",
    "some_name = dataset[\"Column_title\"].cat.codes.values\n",
    "some_other...\n",
    "categorical data = np.stack([some_name, some_other], axis=1) #array of values\n",
    "\n",
    "# Convert to tensor\n",
    "categorical_data = torch.tensor(categorical_data, dtype=torch.int64)\n",
    "\n",
    "# Repeat for numerical columns\n",
    "numerical_data = np.stack([dataset[col].values for col in numerical_cols], 1)\n",
    "numerical_data = torch.tensor(numerical_data, dtype=torch.float)\n",
    "\n",
    "# And output array, GOTTA FLATTEN for tensor function\n",
    "outputs = torch.tensor(dataset[outputs].values.flatten())\n",
    "\n",
    "# Check them all\n",
    "print(categorical_data.shape)\n",
    "print(numerical_data.shape)\n",
    "print(outputs.shape)\n",
    "\n",
    "# Split them up\n",
    "torch.utils.data.random_split(dataset, lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **Creating a combination of modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential combines some Modules\n",
    "model = torch.nn.Sequential(torch.nn.Linear(3, 1), torch.nn.Flatten(0, 1))\n",
    "\n",
    "# Linear Module computs output from input using a linear function\n",
    "# Holds weights and biases internally\n",
    "\n",
    "# Flatten Module flatens output of the linear layer to a 1D tensor\n",
    "# to match shape of y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(....)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimiser** (optim)\n",
    "\n",
    "We have updated weights manually by using \"with torch.no_grad\".\n",
    "When it gets complicated this is a burden. The **optim** package deals with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the optimizer, and link it to our 'model'\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# We will want to zero the gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Then step the Optimzer to update\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remember\n",
    "\n",
    "* Wrap in torch.no_grad() when updating the weights! Then zero the gradients after updating the weights (or gradients will keep amassing), or zero before!\n",
    "\n",
    "* You can just call model(X) since it overrides the call function. I.e. y_pred = model(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autograd function**\n",
    "\n",
    "It is really just 2 functions. \n",
    "\n",
    "1. Forward - computes output tensors from input tensors\n",
    "\n",
    "2. Backward - computs gradient of input tensors wrt values for a given output gradient\n",
    "\n",
    "So we can create a class which inherits torch.autograd.Function and define @staticmethod and forward and backward passes.\n",
    "\n",
    "**NN** -\n",
    "When building networks with lots of layers and learnable parameters we use the nn package to build neural networks. This defines a set of **Modules**, which are roughly equivalent to neureal network layers. The package defines useful loss functions commonly used for training.\n",
    "\n",
    "**Module**  - \n",
    "Receives input Tensors and computes output Tensors, but also holds information about state (such as parameters). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom nn Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
